# Analysis of financial time series data via an ensemble from an LSTM neural network, an ARIMA model and a standard Random Forest

A **time-series** is an ordered sequence of observations the temporal dependence of which reveals trend, seasonality and shock propagation. The goal of time series analysis is to investigate the path of observations of time series and to build models to describe the structure of data and then to predict the future values of the series. When the observations are asset prices, returns or liquidity metrics it becomes a **financial time series**, forming the empirical base of valuation, risk control, and policy surveillance.

**Linear models** such as **ARIMA** offer statistical transparency but assume stationarity and short-range memory, constraints routinely violated by markets that exhibit regime shifts, volatility clustering and non-linear feedback. **Deep-learning approaches**, especially **recurrent neural networks (RNNs)**, discard these assumptions by learning high-order, non-linear mappings directly from data.

The **Long Short-Term Memory (LSTM)** architecture augments RNNs with gated cells that preserve information over hundreds of lags, enabling it to capture dynamics that ARIMA cannot without manual lag inflation or hybrid extensions (like an ARIMA-GARCH stack). (Note that GARCH stands f) Empirical studies show well-regularised LSTMs deliver lower forecast error and more coherent risk estimates than such hybrid models, particularly when heterogeneous signals (i.e. information beyond the raw price series like order flow, macro news, technical factors etc.) are fed into the network. Nonetheless, their performance hinges on ample data, robust cross-validation and strict out-of-sample testing.

Despite their limitations, **ARIMA models** remain foundational in time series forecasting due to their interpretability, fast estimation and suitability for short-memory processes. They provide a reliable benchmark and often outperform more complex models on small or well-behaved datasets, especially when domain-specific tuning (e.g. differencing, AIC selection etc.) is applied.

On the other hand, a **Random Forest** is an ensemble‐learning algorithm that builds a large collection of decision-tree regressors, each trained on a bootstrapped sample of the data and a random subset of input features and averages their predictions to produce a robust, low-variance forecast. In general, Random Forests are fast to fit and can be quickly updated with new trees. When applied to time-series problems, the series is first reframed as a supervised-learning table. The forest then learns non-linear, non-parametric relationships between those engineered features and the next-period price or return, making it popular for financial markets where regimes and interactions shift abruptly. Unlike an **ARIMA model**, which assumes a linear stochastic structure and focuses on capturing autocorrelation with a small set of coefficients, a **Random Forest** makes no stationarity or linearity assumptions.  Moreover, it can be fed by high-dimensional and heterogeneous features coming at the cost of losing interpretability and struggling with long time horizon extrapolation. Compared with an **LSTM network**, the forest is simpler to train, less data-hungry, and immune to vanishing-gradient issues. Yet, it lacks the LSTM’s ability to memorise very long contexts and can be fragile when regime shifts render its fixed lag features obsolete. Another natural shortcoming is that they cannot natively model seasonality and integrate trend explicitly.

Combining models through an **ensemble approach** leverages the complementary strengths of each: ARIMA captures linear, short-run dependencies, LSTM models non-linear, longer-run interactions while the Random forrest maps tree-based non-linearity well. In this notebook, we want to check if this can yield a lower error and a more stable performance than any single method. As well-known ensemble forecasts often improve robustness, reduce overfitting, and yield lower error metrics than either model alone, particularly when model disagreement signals structural shifts or uncertainty. The ensemble chosen here is constructed by simply computing the equally weighted average of the predictions of each approach.

In this repository we will experiment with various different implementations of random forests to see if and under which conditions they deliver results comparable in quality to LSTM and ARIMA fits.
